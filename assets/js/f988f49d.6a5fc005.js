"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[98752],{27664:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>l,toc:()=>p});var o=t(85893),a=t(11151);const i={custom_edit_url:"https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_openlit.ipynb",description:"Use OpenLIT to easily monitor AI agents in production with OpenTelemetry.",source_notebook:"/notebook/agentchat_openlit.ipynb",tags:["integration","monitoring","observability","debugging"],title:"Agent Observability with OpenLIT"},s="Agent Observability with OpenLIT",l={id:"notebooks/agentchat_openlit",title:"Agent Observability with OpenLIT",description:"Use OpenLIT to easily monitor AI agents in production with OpenTelemetry.",source:"@site/docs/notebooks/agentchat_openlit.mdx",sourceDirName:"notebooks",slug:"/notebooks/agentchat_openlit",permalink:"/ag2/docs/notebooks/agentchat_openlit",draft:!1,unlisted:!1,editUrl:"https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_openlit.ipynb",tags:[{label:"integration",permalink:"/ag2/docs/tags/integration"},{label:"monitoring",permalink:"/ag2/docs/tags/monitoring"},{label:"observability",permalink:"/ag2/docs/tags/observability"},{label:"debugging",permalink:"/ag2/docs/tags/debugging"}],version:"current",frontMatter:{custom_edit_url:"https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_openlit.ipynb",description:"Use OpenLIT to easily monitor AI agents in production with OpenTelemetry.",source_notebook:"/notebook/agentchat_openlit.ipynb",tags:["integration","monitoring","observability","debugging"],title:"Agent Observability with OpenLIT"},sidebar:"notebooksSidebar",previous:{title:"Auto Generated Agent Chat: GPTAssistant with Code Interpreter",permalink:"/ag2/docs/notebooks/agentchat_oai_code_interpreter"},next:{title:"Auto Generated Agent Chat: Collaborative Task Solving with Coding and Planning Agent",permalink:"/ag2/docs/notebooks/agentchat_planning"}},r={},p=[{value:"Adding OpenLIT to an existing AutoGen (Now AG2) service",id:"adding-openlit-to-an-existing-autogen-now-ag2-service",level:2},{value:"Lets look at a simple chat example",id:"lets-look-at-a-simple-chat-example",level:2},{value:"Deploy OpenLIT Stack",id:"deploy-openlit-stack",level:2},{value:"Visualize and Optimize!",id:"visualize-and-optimize",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"agent-observability-with-openlit",children:"Agent Observability with OpenLIT"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://colab.research.google.com/github/ag2ai/ag2/blob/main/notebook/agentchat_openlit.ipynb",children:(0,o.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,o.jsx)(n.a,{href:"https://github.com/ag2ai/ag2/blob/main/notebook/agentchat_openlit.ipynb",children:(0,o.jsx)(n.img,{src:"https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github",alt:"Open on GitHub"})})]}),"\n",(0,o.jsx)("img",{src:"https://github.com/openlit/.github/blob/main/profile/assets/wide-logo-no-bg.png?raw=true",alt:"OpenLIT Logo for LLM Observability",width:"30%"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://github.com/openlit/openlit",children:"OpenLIT"})," an open source product\nthat helps developers build and manage AI agents in production,\neffectively helping them improve accuracy. As a self-hosted solution, it\nenables developers to experiment with LLMs, manage and version prompts,\nsecurely manage API keys, and provide safeguards against prompt\ninjection and jailbreak attempts. It also includes built-in\nOpenTelemetry-native observability and evaluation for the complete GenAI\nstack (LLMs, Agents, vector databases, and GPUs)."]}),"\n",(0,o.jsxs)(n.p,{children:["For more info, check out the ",(0,o.jsx)(n.a,{href:"https://github.com/openlit/openlit",children:"OpenLIT\nRepo"})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.img,{src:"https://github.com/openlit/.github/blob/main/profile/assets/openlit-client-1.png?raw=true",alt:""}),"\n",(0,o.jsx)(n.img,{src:"https://github.com/openlit/.github/blob/main/profile/assets/openlit-client-2.png?raw=true",alt:""})]}),"\n",(0,o.jsx)(n.h2,{id:"adding-openlit-to-an-existing-autogen-now-ag2-service",children:"Adding OpenLIT to an existing AutoGen (Now AG2) service"}),"\n",(0,o.jsx)(n.p,{children:"To get started, you\u2019ll need to install the OpenLIT library"}),"\n",(0,o.jsx)(n.p,{children:"OpenLIT uses OpenTelemetry to automatically intrument the AI Agent app\nwhen it\u2019s initialized meaning your agent observability data like\nexecution traces and metrics will be tracked in just one line of code."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"pip install ag2 openlit\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"Collecting ag2\n  Downloading ag2-0.4-py3-none-any.whl.metadata (24 kB)\nCollecting openlit\n  Downloading openlit-1.32.4-py3-none-any.whl.metadata (22 kB)\nCollecting diskcache (from ag2)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nCollecting docker (from ag2)\n  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\nCollecting flaml (from ag2)\n  Downloading FLAML-2.3.2-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: openai>=1.3 in /usr/local/lib/python3.10/dist-packages (from ag2) (1.54.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ag2) (24.2)\nRequirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from ag2) (2.9.2)\nCollecting python-dotenv (from ag2)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ag2) (2.5.0)\nCollecting tiktoken (from ag2)\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ag2) (1.26.4)\nCollecting anthropic<0.22.0,>=0.21.0 (from openlit)\n  Downloading anthropic-0.21.3-py3-none-any.whl.metadata (17 kB)\nCollecting boto3<2.0.0,>=1.34.0 (from openlit)\n  Downloading boto3-1.35.69-py3-none-any.whl.metadata (6.7 kB)\nCollecting botocore<2.0.0,>=1.34.0 (from openlit)\n  Downloading botocore-1.35.69-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: opentelemetry-api<2.0.0,>=1.27.0 in /usr/local/lib/python3.10/dist-packages (from openlit) (1.28.2)\nCollecting opentelemetry-exporter-otlp<2.0.0,>=1.27.0 (from openlit)\n  Downloading opentelemetry_exporter_otlp-1.28.2-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation<0.49,>=0.48b0 (from openlit)\n  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: opentelemetry-sdk<2.0.0,>=1.27.0 in /usr/local/lib/python3.10/dist-packages (from openlit) (1.28.2)\nRequirement already satisfied: requests<3.0.0,>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from openlit) (2.32.3)\nCollecting schedule<2.0.0,>=1.2.2 (from openlit)\n  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\nCollecting tiktoken (from ag2)\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting xmltodict<0.14.0,>=0.13.0 (from openlit)\n  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.22.0,>=0.21.0->openlit) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.22.0,>=0.21.0->openlit) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.22.0,>=0.21.0->openlit) (0.27.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<0.22.0,>=0.21.0->openlit) (1.3.1)\nRequirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.22.0,>=0.21.0->openlit) (0.20.3)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<0.22.0,>=0.21.0->openlit) (4.12.2)\nCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->openlit)\n  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->openlit)\n  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<2.0.0,>=1.34.0->openlit) (2.8.2)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<2.0.0,>=1.34.0->openlit) (2.2.3)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->ag2) (0.7.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->ag2) (4.66.6)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.27.0->openlit) (1.2.15)\nRequirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<2.0.0,>=1.27.0->openlit) (8.5.0)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.28.2 (from opentelemetry-exporter-otlp<2.0.0,>=1.27.0->openlit)\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-exporter-otlp-proto-http==1.28.2 (from opentelemetry-exporter-otlp<2.0.0,>=1.27.0->openlit)\n  Downloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp<2.0.0,>=1.27.0->openlit) (1.66.0)\nRequirement already satisfied: grpcio<2.0.0,>=1.63.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp<2.0.0,>=1.27.0->openlit) (1.68.0)\nCollecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp<2.0.0,>=1.27.0->openlit)\n  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp<2.0.0,>=1.27.0->openlit)\n  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\nCollecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.28.2->opentelemetry-exporter-otlp-proto-grpc==1.28.2->opentelemetry-exporter-otlp<2.0.0,>=1.27.0->openlit)\n  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nRequirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation<0.49,>=0.48b0->openlit) (75.1.0)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation<0.49,>=0.48b0->openlit) (1.16.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<2.0.0,>=1.27.0->openlit) (0.49b2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->ag2) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->ag2) (2.23.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.26.0->openlit) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.26.0->openlit) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.26.0->openlit) (2024.8.30)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ag2) (2024.9.11)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<0.22.0,>=0.21.0->openlit) (1.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<0.22.0,>=0.21.0->openlit) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.22.0,>=0.21.0->openlit) (0.14.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api<2.0.0,>=1.27.0->openlit) (3.21.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.34.0->openlit) (1.16.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.0->openlit) (0.26.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.0->openlit) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.0->openlit) (2024.10.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.22.0,>=0.21.0->openlit) (6.0.2)\nDownloading ag2-0.4-py3-none-any.whl (366 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 366.1/366.1 kB 8.1 MB/s eta 0:00:00\nDownloading openlit-1.32.4-py3-none-any.whl (229 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 229.1/229.1 kB 15.1 MB/s eta 0:00:00\nDownloading anthropic-0.21.3-py3-none-any.whl (851 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 851.6/851.6 kB 8.5 MB/s eta 0:00:00\nDownloading boto3-1.35.69-py3-none-any.whl (139 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 139.2/139.2 kB 9.5 MB/s eta 0:00:00\nDownloading botocore-1.35.69-py3-none-any.whl (13.0 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 13.0/13.0 MB 57.1 MB/s eta 0:00:00\nDownloading opentelemetry_exporter_otlp-1.28.2-py3-none-any.whl (7.0 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_http-1.28.2-py3-none-any.whl (17 kB)\nDownloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\nDownloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 55.8/55.8 kB 4.3 MB/s eta 0:00:00\nDownloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\nDownloading schedule-1.2.2-py3-none-any.whl (12 kB)\nDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 42.3 MB/s eta 0:00:00\nDownloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 45.5/45.5 kB 3.0 MB/s eta 0:00:00\nDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 147.8/147.8 kB 8.6 MB/s eta 0:00:00\nDownloading FLAML-2.3.2-py3-none-any.whl (313 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 313.9/313.9 kB 19.8 MB/s eta 0:00:00\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\nDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 83.2/83.2 kB 6.9 MB/s eta 0:00:00\nDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 316.6/316.6 kB 21.0 MB/s eta 0:00:00\nInstalling collected packages: xmltodict, schedule, python-dotenv, protobuf, jmespath, flaml, diskcache, tiktoken, opentelemetry-proto, docker, botocore, s3transfer, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-common, boto3, anthropic, ag2, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-exporter-otlp, openlit\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.5\n    Uninstalling protobuf-4.25.5:\n      Successfully uninstalled protobuf-4.25.5\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\nSuccessfully installed ag2-0.4 anthropic-0.21.3 boto3-1.35.69 botocore-1.35.69 diskcache-5.6.3 docker-7.1.0 flaml-2.3.2 jmespath-1.0.1 openlit-1.32.4 opentelemetry-exporter-otlp-1.28.2 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-exporter-otlp-proto-http-1.28.2 opentelemetry-instrumentation-0.48b0 opentelemetry-proto-1.28.2 protobuf-5.28.3 python-dotenv-1.0.1 s3transfer-0.10.4 schedule-1.2.2 tiktoken-0.7.0 xmltodict-0.13.0\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import openlit\n\nfrom autogen import AssistantAgent, UserProxyAgent\n\nopenlit.init()\n"})}),"\n",(0,o.jsx)(n.p,{children:"OpenLIT will now start automatically tracking"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"LLM prompts and completions"}),"\n",(0,o.jsx)(n.li,{children:"Token usage and costs"}),"\n",(0,o.jsx)(n.li,{children:"Agent names and actions"}),"\n",(0,o.jsx)(n.li,{children:"Tool usage"}),"\n",(0,o.jsx)(n.li,{children:"Errors"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"lets-look-at-a-simple-chat-example",children:"Lets look at a simple chat example"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import openlit\n\nopenlit.init()\n"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import os\n\nllm_config = {"model": "gpt-4", "api_key": os.environ["OPENAI_API_KEY"]}\nassistant = AssistantAgent("assistant", llm_config=llm_config)\nuser_proxy = UserProxyAgent("user_proxy", code_execution_config=False)\n\n# Start the chat\nuser_proxy.initiate_chat(\n    assistant,\n    message="Tell me a joke about NVDA and TESLA stock prices.",\n)\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:'user_proxy (to assistant):\n\nTell me a joke about NVDA and TESLA stock prices.\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nWhy don\'t NVDA and TESLA stock prices ever get coffee together?\n\nBecause whenever they heat up, they always take a steep drop before they can cool down! \n\nI hope this brings a smile to your face. Investing in stocks can be a rollercoaster sometimes. Please note that this is humor and doesn\'t reflect the actual dynamics of these companies\' stock prices. TERMINATE.\n\n--------------------------------------------------------------------------------\nReplying as user_proxy. Provide feedback to assistant. Press enter to skip and use auto-reply, or type \'exit\' to end the conversation: hi\nuser_proxy (to assistant):\n\nhi\n\n--------------------------------------------------------------------------------\n{\n    "name": "openai.chat.completions",\n    "context": {\n        "trace_id": "0x35ae5952626492e432ae25af5bf92daa",\n        "span_id": "0x44103383aa51d1b1",\n        "trace_state": "[]"\n    },\n    "kind": "SpanKind.CLIENT",\n    "parent_id": null,\n    "start_time": "2024-11-21T01:53:47.597241Z",\n    "end_time": "2024-11-21T01:53:48.506758Z",\n    "status": {\n        "status_code": "OK"\n    },\n    "attributes": {\n        "telemetry.sdk.name": "openlit",\n        "gen_ai.system": "openai",\n        "gen_ai.operation.name": "chat",\n        "gen_ai.endpoint": "openai.chat.completions",\n        "gen_ai.response.id": "chatcmpl-AVqg7QSYB8CpEN1I5PYT1laBcds9S",\n        "gen_ai.environment": "default",\n        "gen_ai.application_name": "default",\n        "gen_ai.request.model": "gpt-4",\n        "gen_ai.request.top_p": 1.0,\n        "gen_ai.request.max_tokens": -1,\n        "gen_ai.request.user": "",\n        "gen_ai.request.temperature": 1.0,\n        "gen_ai.request.presence_penalty": 0.0,\n        "gen_ai.request.frequency_penalty": 0.0,\n        "gen_ai.request.seed": "",\n        "gen_ai.request.is_stream": false,\n        "gen_ai.usage.input_tokens": 580,\n        "gen_ai.usage.output_tokens": 9,\n        "gen_ai.usage.total_tokens": 589,\n        "gen_ai.response.finish_reasons": [\n            "stop"\n        ],\n        "gen_ai.usage.cost": 0.017939999999999998\n    },\n    "events": [\n        {\n            "name": "gen_ai.content.prompt",\n            "timestamp": "2024-11-21T01:53:48.506257Z",\n            "attributes": {\n                "gen_ai.prompt": "system: You are a helpful AI assistant.\\nSolve tasks using your coding and language skills.\\nIn the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\\n    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\\n    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\\nSolve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\\nWhen using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can\'t modify your code. So do not suggest incomplete code which requires users to modify. Don\'t use a code block if it\'s not intended to be executed by the user.\\nIf you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don\'t include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use \'print\' function for the output when relevant. Check the execution result returned by the user.\\nIf the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can\'t be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\\nWhen you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\\nReply \\"TERMINATE\\" in the end when everything is done.\\n    \\nuser: Tell me a joke about NVDA and TESLA stock prices.\\nassistant: Why don\'t NVDA and TESLA stock prices ever get coffee together?\\n\\nBecause whenever they heat up, they always take a steep drop before they can cool down! \\n\\nI hope this brings a smile to your face. Investing in stocks can be a rollercoaster sometimes. Please note that this is humor and doesn\'t reflect the actual dynamics of these companies\' stock prices. TERMINATE.\\nuser: hi"\n            }\n        },\n        {\n            "name": "gen_ai.content.completion",\n            "timestamp": "2024-11-21T01:53:48.506314Z",\n            "attributes": {\n                "gen_ai.completion": "Hello! How can I assist you today?"\n            }\n        }\n    ],\n    "links": [],\n    "resource": {\n        "attributes": {\n            "telemetry.sdk.language": "python",\n            "telemetry.sdk.name": "openlit",\n            "telemetry.sdk.version": "1.28.2",\n            "service.name": "default",\n            "deployment.environment": "default"\n        },\n        "schema_url": ""\n    }\n}\nassistant (to user_proxy):\n\nHello! How can I assist you today?\n\n--------------------------------------------------------------------------------\nReplying as user_proxy. Provide feedback to assistant. Press enter to skip and use auto-reply, or type \'exit\' to end the conversation: exit\n'})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': 'Tell me a joke about NVDA and TESLA stock prices.', 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Why don't NVDA and TESLA stock prices ever get coffee together?\\n\\nBecause whenever they heat up, they always take a steep drop before they can cool down! \\n\\nI hope this brings a smile to your face. Investing in stocks can be a rollercoaster sometimes. Please note that this is humor and doesn't reflect the actual dynamics of these companies' stock prices. TERMINATE.\", 'role': 'user', 'name': 'assistant'}, {'content': 'hi', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Hello! How can I assist you today?', 'role': 'user', 'name': 'assistant'}], summary='Hello! How can I assist you today?', cost={'usage_including_cached_inference': {'total_cost': 0.03731999999999999, 'gpt-4-0613': {'cost': 0.03731999999999999, 'prompt_tokens': 1068, 'completion_tokens': 88, 'total_tokens': 1156}}, 'usage_excluding_cached_inference': {'total_cost': 0.017939999999999998, 'gpt-4-0613': {'cost': 0.017939999999999998, 'prompt_tokens': 580, 'completion_tokens': 9, 'total_tokens': 589}}}, human_input=['hi', 'exit'])\n"})}),"\n",(0,o.jsx)(n.h1,{id:"sending-traces-and-metrics-to-openlit",children:"Sending Traces and metrics to OpenLIT"}),"\n",(0,o.jsx)(n.p,{children:"By default, OpenLIT generates OpenTelemetry traces and metrics that are\nlogged to your console. To set up a detailed monitoring environment,\nthis guide outlines how to deploy OpenLIT and direct all traces and\nmetrics there. You also have the flexibility to send the telemetry data\nto any OpenTelemetry-compatible endpoint, such as Grafana, Jaeger, or\nDataDog."}),"\n",(0,o.jsx)(n.h2,{id:"deploy-openlit-stack",children:"Deploy OpenLIT Stack"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Clone the OpenLIT Repository"}),"\n",(0,o.jsx)(n.p,{children:"Open your terminal or command line and execute:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:"git clone git@github.com:openlit/openlit.git\n"})}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsx)(n.p,{children:"Host it Yourself with Docker"}),"\n",(0,o.jsx)(n.p,{children:"Deploy and start OpenLIT using the command:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:"docker compose up -d\n"})}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["For instructions on installing in Kubernetes using Helm, refer to the\n",(0,o.jsx)(n.a,{href:"https://docs.openlit.io/latest/installation#kubernetes",children:"Kubernetes Helm installation\nguide"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Configure the telemetry data destination as follows:"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Purpose"}),(0,o.jsx)(n.th,{children:"Parameter/Environment Variable"}),(0,o.jsx)(n.th,{children:"For Sending to OpenLIT"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Send data to an HTTP OTLP endpoint"}),(0,o.jsxs)(n.td,{children:[(0,o.jsx)(n.code,{children:"otlp_endpoint"})," or ",(0,o.jsx)(n.code,{children:"OTEL_EXPORTER_OTLP_ENDPOINT"})]}),(0,o.jsx)(n.td,{children:(0,o.jsx)(n.code,{children:'"http://127.0.0.1:4318"'})})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Authenticate telemetry backends"}),(0,o.jsxs)(n.td,{children:[(0,o.jsx)(n.code,{children:"otlp_headers"})," or ",(0,o.jsx)(n.code,{children:"OTEL_EXPORTER_OTLP_HEADERS"})]}),(0,o.jsx)(n.td,{children:"Not required by default"})]})]})]}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["\ud83d\udca1 Info: If the ",(0,o.jsx)(n.code,{children:"otlp_endpoint"})," or ",(0,o.jsx)(n.code,{children:"OTEL_EXPORTER_OTLP_ENDPOINT"})," is\nnot provided, the OpenLIT SDK will output traces directly to your\nconsole, which is recommended during the development phase."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"visualize-and-optimize",children:"Visualize and Optimize!"}),"\n",(0,o.jsx)(n.p,{children:"With the Observability data now being collected and sent to OpenLIT, the\nnext step is to visualize and analyze this data to get insights into\nyour AI application\u2019s performance, behavior, and identify areas of\nimprovement."}),"\n",(0,o.jsxs)(n.p,{children:["Just head over to OpenLIT at ",(0,o.jsx)(n.code,{children:"127.0.0.1:3000"})," on your browser to start\nexploring. You can login using the default credentials - ",(0,o.jsx)(n.strong,{children:"Email"}),":\n",(0,o.jsx)(n.code,{children:"user@openlit.io"})," - ",(0,o.jsx)(n.strong,{children:"Password"}),": ",(0,o.jsx)(n.code,{children:"openlituser"})]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.img,{src:"https://github.com/openlit/.github/blob/main/profile/assets/openlit-ag2-1.png?raw=true",alt:""}),"\n",(0,o.jsx)(n.img,{src:"https://github.com/openlit/.github/blob/main/profile/assets/openlit-ag2-2.png?raw=true",alt:""})]})]})}function d(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>l,a:()=>s});var o=t(67294);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);