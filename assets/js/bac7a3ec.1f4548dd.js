"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[84526],{7515:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var s=n(74848),o=n(28453);const a={custom_edit_url:"https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_structured_outputs.ipynb",description:"OpenAI offers a functionality for defining a structure of the messages generated by LLMs, AutoGen enables this functionality by propagating response_format passed to your agents to the underlying client.",source_notebook:"/notebook/agentchat_structured_outputs.ipynb",tags:["structured output"],title:"Structured output"},i="Structured output",r={id:"notebooks/agentchat_structured_outputs",title:"Structured output",description:"OpenAI offers a functionality for defining a structure of the messages generated by LLMs, AutoGen enables this functionality by propagating response_format passed to your agents to the underlying client.",source:"@site/docs/notebooks/agentchat_structured_outputs.mdx",sourceDirName:"notebooks",slug:"/notebooks/agentchat_structured_outputs",permalink:"/ag2/docs/notebooks/agentchat_structured_outputs",draft:!1,unlisted:!1,editUrl:"https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_structured_outputs.ipynb",tags:[{label:"structured output",permalink:"/ag2/docs/tags/structured-output"}],version:"current",frontMatter:{custom_edit_url:"https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_structured_outputs.ipynb",description:"OpenAI offers a functionality for defining a structure of the messages generated by LLMs, AutoGen enables this functionality by propagating response_format passed to your agents to the underlying client.",source_notebook:"/notebook/agentchat_structured_outputs.ipynb",tags:["structured output"],title:"Structured output"},sidebar:"notebooksSidebar",previous:{title:"Interactive LLM Agent Dealing with Data Stream",permalink:"/ag2/docs/notebooks/agentchat_stream"},next:{title:"WebSurferAgent",permalink:"/ag2/docs/notebooks/agentchat_surfer"}},l={},c=[{value:"Set your API Endpoint",id:"set-your-api-endpoint",level:2},{value:"Example: math reasoning",id:"example-math-reasoning",level:2},{value:"Define the reasoning model",id:"define-the-reasoning-model",level:3},{value:"Applying the Response Format",id:"applying-the-response-format",level:3},{value:"Define chat actors",id:"define-chat-actors",level:3},{value:"Start the chat",id:"start-the-chat",level:3},{value:"Formatting a response",id:"formatting-a-response",level:2},{value:"Define the reasoning model",id:"define-the-reasoning-model-1",level:3},{value:"Define chat actors and start the chat",id:"define-chat-actors-and-start-the-chat",level:3}];function h(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h1,{id:"structured-output",children:"Structured output"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.a,{href:"https://colab.research.google.com/github/ag2ai/ag2/blob/main/notebook/agentchat_structured_outputs.ipynb",children:(0,s.jsx)(t.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,s.jsx)(t.a,{href:"https://github.com/ag2ai/ag2/blob/main/notebook/agentchat_structured_outputs.ipynb",children:(0,s.jsx)(t.img,{src:"https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github",alt:"Open on GitHub"})})]}),"\n",(0,s.jsxs)(t.p,{children:["OpenAI offers functionality for defining a structure of the messages\ngenerated by LLMs, AG2 enables this functionality by propagating\n",(0,s.jsx)(t.code,{children:"response_format"}),", in the LLM configuration for your agents, to the\nunderlying client. This is currently only supported by OpenAI."]}),"\n",(0,s.jsxs)(t.p,{children:["For more info on structured output, please check\n",(0,s.jsx)(t.a,{href:"https://platform.openai.com/docs/guides/structured-outputs",children:"here"})]}),"\n",(0,s.jsxs)(t.admonition,{title:"Requirements",type:"info",children:[(0,s.jsxs)(t.p,{children:["Install ",(0,s.jsx)(t.code,{children:"ag2"}),":"]}),(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:"pip install ag2\n"})}),(0,s.jsxs)(t.p,{children:["For more information, please refer to the ",(0,s.jsx)(t.a,{href:"/docs/installation/",children:"installation guide"}),"."]})]}),"\n",(0,s.jsx)(t.h2,{id:"set-your-api-endpoint",children:"Set your API Endpoint"}),"\n",(0,s.jsxs)(t.p,{children:["The\n",(0,s.jsx)(t.a,{href:"https://ag2ai.github.io/ag2/docs/reference/oai/openai_utils#config_list_from_json",children:(0,s.jsx)(t.code,{children:"config_list_from_json"})}),"\nfunction loads a list of configurations from an environment variable or\na json file. Structured Output is supported by OpenAI\u2019s models from\ngpt-4-0613 and gpt-3.5-turbo-0613."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'import autogen\n\nconfig_list = autogen.config_list_from_json(\n    "OAI_CONFIG_LIST",\n    filter_dict={\n        "model": ["gpt-4o", "gpt-4o-mini"],\n    },\n)\n'})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"})}),"\n",(0,s.jsx)(t.admonition,{type:"tip",children:(0,s.jsxs)(t.p,{children:["Learn more about configuring LLMs for agents ",(0,s.jsx)(t.a,{href:"/docs/topics/llm_configuration",children:"here"}),"."]})}),"\n",(0,s.jsx)(t.h2,{id:"example-math-reasoning",children:"Example: math reasoning"}),"\n",(0,s.jsx)(t.p,{children:"Using structured output, we can enforce chain-of-thought reasoning in\nthe model to output an answer in a structured, step-by-step way."}),"\n",(0,s.jsx)(t.h3,{id:"define-the-reasoning-model",children:"Define the reasoning model"}),"\n",(0,s.jsx)(t.p,{children:"First we will define the math reasoning model. This model will\nindirectly force the LLM to solve the posed math problems iteratively\nthrough math reasoning steps."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"from pydantic import BaseModel\n\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str\n"})}),"\n",(0,s.jsx)(t.h3,{id:"applying-the-response-format",children:"Applying the Response Format"}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"response_format"})," is added to the LLM configuration and then this\nconfiguration is applied to the agent."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'for config in config_list:\n    config["response_format"] = MathReasoning\n'})}),"\n",(0,s.jsx)(t.h3,{id:"define-chat-actors",children:"Define chat actors"}),"\n",(0,s.jsxs)(t.p,{children:["Now we can define the agents that will solve the posed math problem. We\nwill keep this example simple; we will use a ",(0,s.jsx)(t.code,{children:"UserProxyAgent"})," to input\nthe math problem and an ",(0,s.jsx)(t.code,{children:"AssistantAgent"})," to solve it."]}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"AssistantAgent"})," will be constrained to solving the math problem\nstep-by-step by using the ",(0,s.jsx)(t.code,{children:"MathReasoning"})," response format we defined\nabove."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'llm_config = {"config_list": config_list, "cache_seed": 42}\n\nuser_proxy = autogen.UserProxyAgent(\n    name="User_proxy",\n    system_message="A human admin.",\n    human_input_mode="NEVER",\n)\n\nassistant = autogen.AssistantAgent(\n    name="Math_solver",\n    llm_config=llm_config,  # Response Format is in the configuration\n)\n'})}),"\n",(0,s.jsx)(t.h3,{id:"start-the-chat",children:"Start the chat"}),"\n",(0,s.jsxs)(t.p,{children:["Let\u2019s now start the chat and prompt the assistant to solve a simple\nequation. The assistant agent should return a response solving the\nequation using a step-by-step ",(0,s.jsx)(t.code,{children:"MathReasoning"})," model."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'user_proxy.initiate_chat(assistant, message="how can I solve 8x + 7 = -23", max_turns=1, summary_method="last_msg")\n'})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:'User_proxy (to Math_solver):\n\nhow can I solve 8x + 7 = -23\n\n--------------------------------------------------------------------------------\nMath_solver (to User_proxy):\n\n{"steps":[{"explanation":"To isolate the term with x, we first subtract 7 from both sides of the equation.","output":"8x + 7 - 7 = -23 - 7 -> 8x = -30."},{"explanation":"Now that we have 8x = -30, we divide both sides by 8 to solve for x.","output":"x = -30 / 8 -> x = -3.75."}],"final_answer":"x = -3.75"}\n\n--------------------------------------------------------------------------------\n'})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': 'how can I solve 8x + 7 = -23', 'role': 'assistant', 'name': 'User_proxy'}, {'content': '{\"steps\":[{\"explanation\":\"To isolate the term with x, we first subtract 7 from both sides of the equation.\",\"output\":\"8x + 7 - 7 = -23 - 7 -> 8x = -30.\"},{\"explanation\":\"Now that we have 8x = -30, we divide both sides by 8 to solve for x.\",\"output\":\"x = -30 / 8 -> x = -3.75.\"}],\"final_answer\":\"x = -3.75\"}', 'role': 'user', 'name': 'Math_solver'}], summary='{\"steps\":[{\"explanation\":\"To isolate the term with x, we first subtract 7 from both sides of the equation.\",\"output\":\"8x + 7 - 7 = -23 - 7 -> 8x = -30.\"},{\"explanation\":\"Now that we have 8x = -30, we divide both sides by 8 to solve for x.\",\"output\":\"x = -30 / 8 -> x = -3.75.\"}],\"final_answer\":\"x = -3.75\"}', cost={'usage_including_cached_inference': {'total_cost': 0.00015089999999999998, 'gpt-4o-mini-2024-07-18': {'cost': 0.00015089999999999998, 'prompt_tokens': 582, 'completion_tokens': 106, 'total_tokens': 688}}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])\n"})}),"\n",(0,s.jsx)(t.h2,{id:"formatting-a-response",children:"Formatting a response"}),"\n",(0,s.jsxs)(t.p,{children:["When defining a ",(0,s.jsx)(t.code,{children:"response_format"}),", you have the flexibility to customize\nhow the output is parsed and presented, making it more user-friendly. To\ndemonstrate this, we\u2019ll add a ",(0,s.jsx)(t.code,{children:"format"})," method to our ",(0,s.jsx)(t.code,{children:"MathReasoning"}),"\nmodel. This method will define the logic for transforming the raw JSON\nresponse into a more human-readable and accessible format."]}),"\n",(0,s.jsx)(t.h3,{id:"define-the-reasoning-model-1",children:"Define the reasoning model"}),"\n",(0,s.jsxs)(t.p,{children:["Let\u2019s redefine the ",(0,s.jsx)(t.code,{children:"MathReasoning"})," model to include a ",(0,s.jsx)(t.code,{children:"format"})," method.\nThis method will allow the underlying client to parse the return value\nfrom the LLM into a more human-readable format. If the ",(0,s.jsx)(t.code,{children:"format"})," method\nis not defined, the client will default to returning the model\u2019s JSON\nrepresentation, as demonstrated in the previous example."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'from pydantic import BaseModel\n\n\nclass Step(BaseModel):\n    explanation: str\n    output: str\n\n\nclass MathReasoning(BaseModel):\n    steps: list[Step]\n    final_answer: str\n\n    def format(self) -> str:\n        steps_output = "\\n".join(\n            f"Step {i + 1}: {step.explanation}\\n  Output: {step.output}" for i, step in enumerate(self.steps)\n        )\n        return f"{steps_output}\\n\\nFinal Answer: {self.final_answer}"\n'})}),"\n",(0,s.jsx)(t.h3,{id:"define-chat-actors-and-start-the-chat",children:"Define chat actors and start the chat"}),"\n",(0,s.jsx)(t.p,{children:"The rest of the process is the same as in the previous example: define\nthe actors and start the chat."}),"\n",(0,s.jsxs)(t.p,{children:["Observe how the Math_solver agent now communicates using the format we\nhave defined in our ",(0,s.jsx)(t.code,{children:"MathReasoning.format"})," method."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'for config in config_list:\n    config["response_format"] = MathReasoning\n\nuser_proxy = autogen.UserProxyAgent(\n    name="User_proxy",\n    system_message="A human admin.",\n    human_input_mode="NEVER",\n)\n\nassistant = autogen.AssistantAgent(\n    name="Math_solver",\n    llm_config=llm_config,\n)\n\nuser_proxy.initiate_chat(assistant, message="how can I solve 8x + 7 = -23", max_turns=1, summary_method="last_msg")\n'})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"User_proxy (to Math_solver):\n\nhow can I solve 8x + 7 = -23\n\n--------------------------------------------------------------------------------\nMath_solver (to User_proxy):\n\nStep 1: To isolate the term with x, we first subtract 7 from both sides of the equation.\n  Output: 8x + 7 - 7 = -23 - 7 -> 8x = -30.\nStep 2: Now that we have 8x = -30, we divide both sides by 8 to solve for x.\n  Output: x = -30 / 8 -> x = -3.75.\n\nFinal Answer: x = -3.75\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': 'how can I solve 8x + 7 = -23', 'role': 'assistant', 'name': 'User_proxy'}, {'content': 'Step 1: To isolate the term with x, we first subtract 7 from both sides of the equation.\\n  Output: 8x + 7 - 7 = -23 - 7 -> 8x = -30.\\nStep 2: Now that we have 8x = -30, we divide both sides by 8 to solve for x.\\n  Output: x = -30 / 8 -> x = -3.75.\\n\\nFinal Answer: x = -3.75', 'role': 'user', 'name': 'Math_solver'}], summary='Step 1: To isolate the term with x, we first subtract 7 from both sides of the equation.\\n  Output: 8x + 7 - 7 = -23 - 7 -> 8x = -30.\\nStep 2: Now that we have 8x = -30, we divide both sides by 8 to solve for x.\\n  Output: x = -30 / 8 -> x = -3.75.\\n\\nFinal Answer: x = -3.75', cost={'usage_including_cached_inference': {'total_cost': 0.00015089999999999998, 'gpt-4o-mini-2024-07-18': {'cost': 0.00015089999999999998, 'prompt_tokens': 582, 'completion_tokens': 106, 'total_tokens': 688}}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])\n"})}),"\n",(0,s.jsx)(t.p,{children:"Normal function calling still works alongside structured output, so your\nagent can have a response format while still calling tools."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:'@assistant.register_for_execution()\n@assistant.register_for_llm(description="You can use this function call to solve addition")\ndef add(x: int, y: int) -> int:\n    return x + y\n\n\nuser_proxy.initiate_chat(\n    assistant, message="solve 3 + 4 by calling appropriate function", max_turns=1, summary_method="last_msg"\n)\n'})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:'User_proxy (to Math_solver):\n\nsolve 3 + 4 by calling appropriate function\n\n--------------------------------------------------------------------------------\nMath_solver (to User_proxy):\n\n***** Suggested tool call (call_oTp96rVzs2kAOwGhBM5rJDcW): add *****\nArguments: \n{"x":3,"y":4}\n********************************************************************\n\n--------------------------------------------------------------------------------\n'})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': 'solve 3 + 4 by calling appropriate function', 'role': 'assistant', 'name': 'User_proxy'}, {'tool_calls': [{'id': 'call_oTp96rVzs2kAOwGhBM5rJDcW', 'function': {'arguments': '{\"x\":3,\"y\":4}', 'name': 'add'}, 'type': 'function'}], 'content': None, 'role': 'assistant'}], summary='', cost={'usage_including_cached_inference': {'total_cost': 0.0001029, 'gpt-4o-mini-2024-07-18': {'cost': 0.0001029, 'prompt_tokens': 618, 'completion_tokens': 17, 'total_tokens': 635}}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])\n"})})]})}function d(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>r});var s=n(96540);const o={},a=s.createContext(o);function i(e){const t=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(a.Provider,{value:t},e.children)}}}]);